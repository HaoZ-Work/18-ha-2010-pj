{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819d9d7-f1cc-4749-9b40-a131d51c2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59320008-9079-4c99-9706-823b54d955f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7015ee81-c174-400c-a99c-09420c26d0d6",
   "metadata": {},
   "source": [
    "## Load data and create meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27193c2c-ed51-44b3-bb66-51d3badd31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = \"training/\"\n",
    "# training_data_dir = \"/home/kurse/kurs00056/hz53kahe/physionet.org/files/challenge-2017/1.0.0/training2017/\"\n",
    "\n",
    "training_data_path = glob(training_data_dir+\"*mat\")\n",
    "training_path_id_dic = { x.split('/')[-1].split('.')[0] : x for x in training_data_path}\n",
    "\n",
    "\n",
    "reference = pd.read_csv(training_data_dir+\"REFERENCE.csv\",header=None)\n",
    "reference = reference.rename(columns={0:'id',1:\"label\"})\n",
    "reference_dic = dict(zip(reference['id'].to_list(),reference['label'].to_list() ) )\n",
    "\n",
    "meta_pd = pd.DataFrame(columns=[\"id\",\"path\",\"label\"])\n",
    "meta_pd['id'] = training_path_id_dic.keys()\n",
    "meta_pd['path'] = meta_pd['id'].map( training_path_id_dic.get )\n",
    "meta_pd['label'] = meta_pd['id'].map(  reference_dic.get )\n",
    "meta_pd['encoded_label'] = pd.Categorical(meta_pd['label']).codes\n",
    "\n",
    "def get_mat(mat_path):\n",
    "    s = sio.loadmat(mat_path)\n",
    "    return s['val'][0]\n",
    "\n",
    "meta_pd['data'] = meta_pd['path'].map( get_mat )\n",
    "meta_pd['mean'] = meta_pd['data'].map( np.mean )\n",
    "meta_pd['std'] = meta_pd['data'].map( np.std )\n",
    "meta_pd['length'] = meta_pd['data'].map( np.shape )\n",
    "# meta_pd['length'] = meta_pd['length'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b257d26-effb-4577-90db-adb6f5f0bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_pd.tail(30)\n",
    "# # meta_pd['length'][0]\n",
    "# meta_pd[meta_pd['id']=='train_ecg_02130' ]['data'].to_list()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f6609-5402-448f-8bed-5e2e55e146cd",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d6e1ab-794c-4061-9b4d-193d9ab69206",
   "metadata": {},
   "source": [
    "### ratio of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334c4119-ff18-4278-b847-77d07cb8e963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'A': 738, 'N': 5050, 'O': 2456, '~': 284}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cates = pd.Categorical(meta_pd['label'], ordered=True).categories\n",
    "cate_counts = meta_pd['encoded_label'].value_counts().to_list()\n",
    "\n",
    "cate_counts_dic = dict(zip(cates, [len(meta_pd[meta_pd['label']==i]) for i in cates]))\n",
    "\n",
    "plt.pie(cate_counts_dic.values(), labels = cate_counts_dic.keys(), colors = sns.color_palette('pastel')[0:4], autopct='%.0f%%')\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.bar(cate_counts_dic.keys(), cate_counts_dic.values())\n",
    "\n",
    "\n",
    "# for i  in cates:\n",
    "#     print(i,len(meta_pd[meta_pd['label']==i]))\n",
    "\n",
    "# # print(cates)\n",
    "# # print(cate_counts)\n",
    "cate_counts_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550b243-35e8-4adb-8dc5-c35aaab38e96",
   "metadata": {},
   "source": [
    "### length of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcf12d0-00b9-47d5-8060-66395f8495dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8528"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = meta_pd[\"length\"].max()[0] #18286\n",
    "MIN_LEN= meta_pd[\"length\"].min()[0] ## 2714\n",
    "len(meta_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47878c-4c59-4315-9033-685b87ec053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test board for add new preprocessing components\n",
    "# a plot for single training sample\n",
    "sample_np = meta_pd[ meta_pd['id']=='train_ecg_04834' ].data.to_list()[0] # to get a array form data\n",
    "# meta_pd[ meta_pd['id']=='train_ecg_00775' ].data.to_list()[0].shape[0]\n",
    "\n",
    "# plt.plot(range(sample_np.shape[0]),sample_np)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_np_pad=  np.concatenate( (sample_np, np.ones(  MAX_LEN-sample_np.shape[0] ) *sample_np.mean()   ))\n",
    "sample_np_pad=  np.pad(sample_np, (0,MAX_LEN-sample_np.shape[0]),  'reflect')\n",
    "\n",
    "\n",
    "# print(len(sample_np_pad))\n",
    "# plt.plot(range(sample_np_pad.shape[0]),sample_np_pad)\n",
    "\n",
    "\n",
    "# # sample_np = preprocessing.Normalizer().fit_transform(sample_np.reshape(1, -1))\n",
    "# # plt.plot(range(7302),sample_np.reshape(7302))\n",
    "\n",
    "# # a = np.array([1,2,3])\n",
    "# # b= np.pad(a,(0,15), 'wrap')\n",
    "# # print( b )\n",
    "\n",
    "sample_np_fft = np.abs( np.fft.rfft(sample_np_pad) )\n",
    "sample_np_fft = preprocessing.Normalizer().fit_transform(sample_np_fft.reshape(1, -1))[0]\n",
    "print(sample_np_fft.std())\n",
    "# sample_np_fft = preprocessing.StandardScaler().fit_transform(sample_np_fft.reshape(1, -1))\n",
    "sample_np_fft = (sample_np_fft-sample_np_fft.mean())/ sample_np_fft.std()\n",
    "print((sample_np_fft.std()))\n",
    "sample_np_fft.shape\n",
    "plt.plot(range(sample_np_fft.shape[0]),sample_np_fft)\n",
    "\n",
    "a= np.array([1,2,3,4,5])\n",
    "a[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f5ff2-05c6-46ed-ae3a-66623139008d",
   "metadata": {},
   "source": [
    "### Perform Data preprocessing and store them in given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7452d0-fb94-4793-bd5f-473b70356cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -89  -98 -101 ...  -92  -67  -40]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def data_len_norm(meta):\n",
    "    '''\n",
    "    Normalize the lengh of data follwing the idea from paper:\n",
    "    https://res.mdpi.com/d_attachment/sensors/sensors-20-02136/article_deploy/sensors-20-02136.pdf\n",
    "    \n",
    "    Args:\n",
    "\n",
    "        meta: pd.Dataframe, meta date in pd form.\n",
    "    \n",
    "    return:\n",
    "        re: meta data after  processing\n",
    "    \n",
    "    '''\n",
    "   \n",
    "  \n",
    "    \n",
    "    re_pd = meta.copy()\n",
    "    #print(len(re_pd))\n",
    "    n = 0\n",
    "    for row in re_pd.iterrows():\n",
    "        if row[1]['length'][0] > 9000:\n",
    "            adding_list = cut_recording(row[1]['data'])\n",
    "            re_pd = re_pd.drop(row[0])\n",
    "            for n in adding_list:\n",
    "                new_recording = meta.iloc[row[0]].copy()\n",
    "                new_recording['data'] = n\n",
    "                #new_recording['id'] = 'extra_'+new_recording['id']\n",
    "                new_recording['length'] = 9000\n",
    "                # print(new_recording)\n",
    "                re_pd = re_pd.append(new_recording,ignore_index = True)\n",
    "                n+=1\n",
    "               \n",
    "\n",
    "            \n",
    "      \n",
    "        if row[1]['length'][0] < 9000:\n",
    "            pass\n",
    "\n",
    "    print(n)\n",
    "    return re_pd\n",
    "\n",
    "\n",
    "LENGTH_THRESHOLD = 9000\n",
    "\n",
    "\n",
    "def cut_recording(ecg:np.array,length_threshold=9000):\n",
    "    len_overlap = int(0.5*length_threshold)\n",
    "    start = 0\n",
    "    end = 9000\n",
    "    re = []\n",
    "    re.append(ecg[start:end])\n",
    "    while True:\n",
    "        if end+len_overlap > len(ecg):\n",
    "            re.append(ecg[-9000:])\n",
    "            break\n",
    "        re.append(ecg[start+len_overlap:end+len_overlap])\n",
    "        start +=len_overlap\n",
    "        end+= len_overlap\n",
    "        \n",
    "        \n",
    "    # print(ecg)\n",
    "    \n",
    "   \n",
    "    return re\n",
    "\n",
    "\n",
    "tmp_pd = data_len_norm(meta_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8636b7e8-ce92-4790-bcc6-b0105f25a7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>data</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, path, label, encoded_label, data, mean, std, length]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tmp_pd.columns\n",
    "meta_pd =tmp_pd \n",
    "meta_pd.columns\n",
    "len(meta_pd)\n",
    "meta_pd[meta_pd['id']=='extra_train_ecg_02130' ]\n",
    "# meta_pd[meta_pd['id']=='train_ecg_02130' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114fa6f1-c11c-4615-8d7b-e10eef69491e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'A': 1047, 'N': 6764, 'O': 3890, '~': 299}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cates = pd.Categorical(meta_pd['label'], ordered=True).categories\n",
    "cate_counts = meta_pd['encoded_label'].value_counts().to_list()\n",
    "\n",
    "cate_counts_dic = dict(zip(cates, [len(meta_pd[meta_pd['label']==i]) for i in cates]))\n",
    "\n",
    "plt.pie(cate_counts_dic.values(), labels = cate_counts_dic.keys(), colors = sns.color_palette('pastel')[0:4], autopct='%.0f%%')\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.bar(cate_counts_dic.keys(), cate_counts_dic.values())\n",
    "\n",
    "\n",
    "# for i  in cates:\n",
    "#     print(i,len(meta_pd[meta_pd['label']==i]))\n",
    "\n",
    "# # print(cates)\n",
    "# # print(cate_counts)\n",
    "cate_counts_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d52ebe0-72fb-46eb-b440-c98e5abd26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(meta=meta_pd,to_path='precessed_data/normalized/',func_list=[],store = False):\n",
    "    '''\n",
    "    Perfrom the preprocessing to the data from given path and store the precessed data in given path\n",
    "    \n",
    "    Args:\n",
    "        meta: like meta_pd which we created before\n",
    "        from_path:  path contains *.mat data\n",
    "        to_path: path where store the precessed data in npy form \n",
    "        func_list : aprroaches , which will be performed on the data.Notice that,  the approaches will be performed in given ordering\n",
    "        store: bool, True to store the data, False for only return tmp_data\n",
    "    return:\n",
    "    '''\n",
    "    # tmp_meta = pd.DataFrame()\n",
    "    # tmp_meta['id'] = meta['id']\n",
    "    \n",
    "    tmp_meta = meta.copy()\n",
    "    tmp_meta['to_path'] = to_path + meta['id']+'.npy'\n",
    "\n",
    "    tmp_meta['preprocessed_data'] = meta['data']\n",
    "    tmp_meta['preprocessed_data_wo_fft'] = meta['data']\n",
    "    for f in func_list:\n",
    "        tmp_meta['preprocessed_data'] = tmp_meta['preprocessed_data'].map(f)\n",
    "        if f != ecg_fourier:\n",
    "            tmp_meta['preprocessed_data_wo_fft'] = tmp_meta['preprocessed_data_wo_fft'].map(f)\n",
    "\n",
    "  \n",
    "    if store==True:\n",
    "        for path,array in zip( tmp_meta['to_path'].to_list(),tmp_meta['preprocessed_data'].to_list()[0]):\n",
    "            np.save( path,array  )\n",
    "    \n",
    "    \n",
    "    #print(tmp_meta.head())\n",
    "    return tmp_meta\n",
    "\n",
    "def ecg_norm( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform normalization\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "    ecg_np = ecg_np.reshape(1,-1)\n",
    "    re = preprocessing.Normalizer().fit_transform(ecg_np)[0]-0.5\n",
    "\n",
    "    return re\n",
    "\n",
    "def ecg_pad( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform padding (with mean by default )\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    re = np.concatenate( ( ecg_np, np.ones(MAX_LEN -ecg_np.shape[0] )  *ecg_np.mean() ))\n",
    " \n",
    "    return re\n",
    "\n",
    "def ecg_pad_repeat( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform padding (with mean by default )\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    re = np.pad(ecg_np , (0,MAX_LEN-ecg_np.shape[0]),  'wrap')\n",
    "    # low the avg scroes\n",
    "    \n",
    "    return re\n",
    "def ecg_len_norm( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to make the length be 9000 (over 9000 then cut, under 9000 then repeat)\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "    if len(ecg_np) < 9000:\n",
    "        re = np.pad(ecg_np , (0,9000-ecg_np.shape[0]),  'wrap')\n",
    "    elif len(ecg_np) >9000 :\n",
    "        re = ecg_np[:9000]\n",
    "        # print(len(ecg_np))\n",
    "    else :\n",
    "        re = ecg_np\n",
    "\n",
    "    \n",
    "    return re\n",
    "\n",
    "def ecg_fourier( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform Fourier Transform\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "   \n",
    "\n",
    "    re = np.abs(np.fft.rfft(ecg_np))\n",
    " \n",
    "    return re\n",
    "\n",
    "\n",
    "def ecg_stand( ecg_np ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform Standardize\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: ecg data in np.array form in shape of (N,)\n",
    "    \n",
    "    return:\n",
    "        re: processed data in array form\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    re = (ecg_np-ecg_np.mean())/ ecg_np.std()\n",
    "    return re\n",
    "\n",
    "def data_ros(meta, cc_dict ):\n",
    "    '''\n",
    "    Be used in preprocess() to perform Rondom over sampling. To address the imbalance problem.\n",
    "    \n",
    "    Args:\n",
    "        cc_dict: dict, for cate_counts_dic\n",
    "        meta: pd.Dataframe, meta date in pd form.\n",
    "    \n",
    "    return:\n",
    "        re: meta data after ros\n",
    "    \n",
    "    '''\n",
    "   \n",
    "  \n",
    "    \n",
    "    data_aug_ratio_dic =dict( zip(cc_dict.keys()  , (np.array([*cc_dict.values()]).max() / np.array([*cc_dict.values()])).round() )   ) \n",
    "    \n",
    "    print(data_aug_ratio_dic)\n",
    "    re_pd = meta.copy()\n",
    "    \n",
    "    for (l,r )in data_aug_ratio_dic.items():\n",
    "        # print(l,r)\n",
    "        re_pd = re_pd.append( [meta.loc[ meta['label'] == l,:]] * int(r-1) ,ignore_index=True)\n",
    "    \n",
    "    print((re_pd['encoded_label'].value_counts().values.max()/re_pd['encoded_label'].value_counts().values).round())\n",
    "\n",
    "    \n",
    "    \n",
    "    return re_pd\n",
    "\n",
    "# preprocess(meta=meta_pd[:3],func_list=[ecg_norm, ecg_pad], store = True)\n",
    "# meta_pd_ros = data_ros(meta_pd,cate_counts_dic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "038572f7-6b7c-43dd-bfa1-02b1b8064720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>data</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>A06819</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>[-31, -31, -31, -32, -32, -32, -32, -32, -32, ...</td>\n",
       "      <td>15.242722</td>\n",
       "      <td>148.126692</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-320, -369, -390, -407, -405, -362, -258, -12...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[492, 366, 248, 143, 50, -25, -79, -111, -124,...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-89, -98, -101, -103, -94, -79, -67, -63, -61...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-89, -98, -101, -103, -94, -79, -67, -63, -61...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               path label  \\\n",
       "11995  A06819  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     O   \n",
       "11996  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11997  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11998  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11999  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "\n",
       "       encoded_label                                               data  \\\n",
       "11995              2  [-31, -31, -31, -32, -32, -32, -32, -32, -32, ...   \n",
       "11996              0  [-320, -369, -390, -407, -405, -362, -258, -12...   \n",
       "11997              0  [492, 366, 248, 143, 50, -25, -79, -111, -124,...   \n",
       "11998              0  [-89, -98, -101, -103, -94, -79, -67, -63, -61...   \n",
       "11999              0  [-89, -98, -101, -103, -94, -79, -67, -63, -61...   \n",
       "\n",
       "            mean         std length  \n",
       "11995  15.242722  148.126692   9000  \n",
       "11996  35.235056  183.789602   9000  \n",
       "11997  35.235056  183.789602   9000  \n",
       "11998  35.235056  183.789602   9000  \n",
       "11999  35.235056  183.789602   9000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_pd.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0303e71a-34f0-478c-93bb-192a468c2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_precessed_pd=None\n",
    "meta_precessed_pd = preprocess(meta=meta_pd,func_list=[ecg_len_norm,ecg_norm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba879592-455f-40b5-9f84-23870432059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>data</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>length</th>\n",
       "      <th>to_path</th>\n",
       "      <th>preprocessed_data</th>\n",
       "      <th>preprocessed_data_wo_fft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A06456</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>[-682, -806, -921, -1014, -1045, -1030, -920, ...</td>\n",
       "      <td>20.717000</td>\n",
       "      <td>194.775029</td>\n",
       "      <td>(9000,)</td>\n",
       "      <td>precessed_data/normalized/A06456.npy</td>\n",
       "      <td>[-0.5367017688310106, -0.5433748177093761, -0....</td>\n",
       "      <td>[-0.5367017688310106, -0.5433748177093761, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A02900</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-169, -184, -220, -269, -326, -382, -409, -44...</td>\n",
       "      <td>80.244111</td>\n",
       "      <td>398.319050</td>\n",
       "      <td>(9000,)</td>\n",
       "      <td>precessed_data/normalized/A02900.npy</td>\n",
       "      <td>[-0.5043842531091519, -0.5047733880004968, -0....</td>\n",
       "      <td>[-0.5043842531091519, -0.5047733880004968, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A04871</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[-218, -275, -338, -401, -457, -500, -517, -52...</td>\n",
       "      <td>3.449667</td>\n",
       "      <td>173.430316</td>\n",
       "      <td>(9000,)</td>\n",
       "      <td>precessed_data/normalized/A04871.npy</td>\n",
       "      <td>[-0.5132472071834246, -0.5167109264928522, -0....</td>\n",
       "      <td>[-0.5132472071834246, -0.5167109264928522, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01704</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[-325, -389, -452, -512, -547, -556, -563, -56...</td>\n",
       "      <td>19.587000</td>\n",
       "      <td>235.235330</td>\n",
       "      <td>(9000,)</td>\n",
       "      <td>precessed_data/normalized/A01704.npy</td>\n",
       "      <td>[-0.5145130682468904, -0.5173710263016627, -0....</td>\n",
       "      <td>[-0.5145130682468904, -0.5173710263016627, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01051</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>[122, 143, 152, 159, 165, 172, 176, 179, 181, ...</td>\n",
       "      <td>9.317444</td>\n",
       "      <td>122.448214</td>\n",
       "      <td>(9000,)</td>\n",
       "      <td>precessed_data/normalized/A01051.npy</td>\n",
       "      <td>[-0.4895279324497183, -0.48772536344516165, -0...</td>\n",
       "      <td>[-0.4895279324497183, -0.48772536344516165, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>A06819</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>[-31, -31, -31, -32, -32, -32, -32, -32, -32, ...</td>\n",
       "      <td>15.242722</td>\n",
       "      <td>148.126692</td>\n",
       "      <td>9000</td>\n",
       "      <td>precessed_data/normalized/A06819.npy</td>\n",
       "      <td>[-0.5028452193578722, -0.5028452193578722, -0....</td>\n",
       "      <td>[-0.5028452193578722, -0.5028452193578722, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-320, -369, -390, -407, -405, -362, -258, -12...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "      <td>precessed_data/normalized/A00613.npy</td>\n",
       "      <td>[-0.5172204041272586, -0.519857278509245, -0.5...</td>\n",
       "      <td>[-0.5172204041272586, -0.519857278509245, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[492, 366, 248, 143, 50, -25, -79, -111, -124,...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "      <td>precessed_data/normalized/A00613.npy</td>\n",
       "      <td>[-0.471606533305089, -0.4788780308732979, -0.4...</td>\n",
       "      <td>[-0.471606533305089, -0.4788780308732979, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-89, -98, -101, -103, -94, -79, -67, -63, -61...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "      <td>precessed_data/normalized/A00613.npy</td>\n",
       "      <td>[-0.5052466137162044, -0.505777170159416, -0.5...</td>\n",
       "      <td>[-0.5052466137162044, -0.505777170159416, -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>A00613</td>\n",
       "      <td>/home/kurse/kurs00056/hz53kahe/physionet.org/f...</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[-89, -98, -101, -103, -94, -79, -67, -63, -61...</td>\n",
       "      <td>35.235056</td>\n",
       "      <td>183.789602</td>\n",
       "      <td>9000</td>\n",
       "      <td>precessed_data/normalized/A00613.npy</td>\n",
       "      <td>[-0.5052466137162044, -0.505777170159416, -0.5...</td>\n",
       "      <td>[-0.5052466137162044, -0.505777170159416, -0.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               path label  \\\n",
       "0      A06456  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     O   \n",
       "1      A02900  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "2      A04871  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     N   \n",
       "3      A01704  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     N   \n",
       "4      A01051  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     N   \n",
       "...       ...                                                ...   ...   \n",
       "11995  A06819  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     O   \n",
       "11996  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11997  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11998  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "11999  A00613  /home/kurse/kurs00056/hz53kahe/physionet.org/f...     A   \n",
       "\n",
       "       encoded_label                                               data  \\\n",
       "0                  2  [-682, -806, -921, -1014, -1045, -1030, -920, ...   \n",
       "1                  0  [-169, -184, -220, -269, -326, -382, -409, -44...   \n",
       "2                  1  [-218, -275, -338, -401, -457, -500, -517, -52...   \n",
       "3                  1  [-325, -389, -452, -512, -547, -556, -563, -56...   \n",
       "4                  1  [122, 143, 152, 159, 165, 172, 176, 179, 181, ...   \n",
       "...              ...                                                ...   \n",
       "11995              2  [-31, -31, -31, -32, -32, -32, -32, -32, -32, ...   \n",
       "11996              0  [-320, -369, -390, -407, -405, -362, -258, -12...   \n",
       "11997              0  [492, 366, 248, 143, 50, -25, -79, -111, -124,...   \n",
       "11998              0  [-89, -98, -101, -103, -94, -79, -67, -63, -61...   \n",
       "11999              0  [-89, -98, -101, -103, -94, -79, -67, -63, -61...   \n",
       "\n",
       "            mean         std   length                               to_path  \\\n",
       "0      20.717000  194.775029  (9000,)  precessed_data/normalized/A06456.npy   \n",
       "1      80.244111  398.319050  (9000,)  precessed_data/normalized/A02900.npy   \n",
       "2       3.449667  173.430316  (9000,)  precessed_data/normalized/A04871.npy   \n",
       "3      19.587000  235.235330  (9000,)  precessed_data/normalized/A01704.npy   \n",
       "4       9.317444  122.448214  (9000,)  precessed_data/normalized/A01051.npy   \n",
       "...          ...         ...      ...                                   ...   \n",
       "11995  15.242722  148.126692     9000  precessed_data/normalized/A06819.npy   \n",
       "11996  35.235056  183.789602     9000  precessed_data/normalized/A00613.npy   \n",
       "11997  35.235056  183.789602     9000  precessed_data/normalized/A00613.npy   \n",
       "11998  35.235056  183.789602     9000  precessed_data/normalized/A00613.npy   \n",
       "11999  35.235056  183.789602     9000  precessed_data/normalized/A00613.npy   \n",
       "\n",
       "                                       preprocessed_data  \\\n",
       "0      [-0.5367017688310106, -0.5433748177093761, -0....   \n",
       "1      [-0.5043842531091519, -0.5047733880004968, -0....   \n",
       "2      [-0.5132472071834246, -0.5167109264928522, -0....   \n",
       "3      [-0.5145130682468904, -0.5173710263016627, -0....   \n",
       "4      [-0.4895279324497183, -0.48772536344516165, -0...   \n",
       "...                                                  ...   \n",
       "11995  [-0.5028452193578722, -0.5028452193578722, -0....   \n",
       "11996  [-0.5172204041272586, -0.519857278509245, -0.5...   \n",
       "11997  [-0.471606533305089, -0.4788780308732979, -0.4...   \n",
       "11998  [-0.5052466137162044, -0.505777170159416, -0.5...   \n",
       "11999  [-0.5052466137162044, -0.505777170159416, -0.5...   \n",
       "\n",
       "                                preprocessed_data_wo_fft  \n",
       "0      [-0.5367017688310106, -0.5433748177093761, -0....  \n",
       "1      [-0.5043842531091519, -0.5047733880004968, -0....  \n",
       "2      [-0.5132472071834246, -0.5167109264928522, -0....  \n",
       "3      [-0.5145130682468904, -0.5173710263016627, -0....  \n",
       "4      [-0.4895279324497183, -0.48772536344516165, -0...  \n",
       "...                                                  ...  \n",
       "11995  [-0.5028452193578722, -0.5028452193578722, -0....  \n",
       "11996  [-0.5172204041272586, -0.519857278509245, -0.5...  \n",
       "11997  [-0.471606533305089, -0.4788780308732979, -0.4...  \n",
       "11998  [-0.5052466137162044, -0.505777170159416, -0.5...  \n",
       "11999  [-0.5052466137162044, -0.505777170159416, -0.5...  \n",
       "\n",
       "[12000 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(meta_precessed_pd)\n",
    "meta_precessed_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406a2b3-5240-4333-be74-8efeb5051878",
   "metadata": {},
   "source": [
    "### Split the data into training , val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd4a0470-70d4-4afd-9a8e-7f7c2ab90714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 6.0, 'N': 1.0, 'O': 2.0, '~': 23.0}\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# meta_precessed_pd = preprocess(meta=meta_pd,func_list=[ecg_pad,ecg_norm]) # get 0.55 and 0.52 in rf for acc and avg f1\n",
    "\n",
    "# meta_precessed_pd = preprocess(meta=meta_pd,func_list=[ecg_pad_repeat,ecg_norm,ecg_fourier]) # get 0.58 and 0.56 on mlp \n",
    "\n",
    "\n",
    "# meta_precessed_pd = preprocess(meta=meta_pd,func_list=[ecg_len_norm,ecg_fourier,ecg_norm,ecg_stand])\n",
    "\n",
    "train_pd,  test_pd = train_test_split(meta_precessed_pd, test_size=0.3, random_state=42,stratify =meta_precessed_pd['encoded_label'] )\n",
    "# print(train_pd.head())\n",
    "\n",
    "\n",
    "train_pd_ros = data_ros(train_pd,cate_counts_dic )\n",
    "\n",
    "X_train_pd = train_pd_ros['preprocessed_data_wo_fft']\n",
    "y_train_pd = train_pd_ros['encoded_label']\n",
    "X_val_pd, X_test_pd, y_val_pd, y_test_pd = train_test_split(test_pd['preprocessed_data_wo_fft'], test_pd['encoded_label'], test_size=0.33, random_state=42,stratify = test_pd['encoded_label'])\n",
    "\n",
    "\n",
    "\n",
    "def pd_2array(X_pd,y_pd):\n",
    "    '''\n",
    "    Convert DataFrema to np.array\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        X_pd:X in datafrema form, each row is one np.array data\n",
    "        y_pd: y in datafrema form, each row is a single number\n",
    "    \n",
    "    return:\n",
    "        X,y in np.array form \n",
    "    '''\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    for X,y in zip( X_pd,y_pd):\n",
    "        X_.append(X.tolist())\n",
    "        y_.append(y) \n",
    "\n",
    "    return np.array(X_),np.array(y_)\n",
    "\n",
    "\n",
    "X_train , y_train = pd_2array(X_train_pd, y_train_pd)\n",
    "X_val , y_val = pd_2array(X_val_pd, y_val_pd)\n",
    "X_test , y_test = pd_2array(X_test_pd, y_test_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f7308-2085-4665-b1b0-306fdc46ba19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])\n",
    "# y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22dc5415-74a6-42fc-b842-182e5453b384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'path', 'label', 'encoded_label', 'data', 'mean', 'std', 'length',\n",
       "       'to_path', 'preprocessed_data', 'preprocessed_data_wo_fft'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_precessed_pd.columns\n",
    "# meta_precessed_pd['preprocessed_data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184b73a-10d7-4d43-b501-0ccc0987059d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ce0d1-63af-429e-993f-4f2ad53d9d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf4e91-0025-4b33-8b9c-ff8fc8af9e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a6d03-ba01-47db-a691-c7a7431d4ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bafe6a-b11e-4a3f-acf1-afc18fdacac6",
   "metadata": {},
   "source": [
    "### Test on some simple ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e0bb6c7-007c-402a-b781-d04a30ec4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91      4398\n",
      "           1       1.00      0.68      0.81      4735\n",
      "           2       0.71      1.00      0.83      5446\n",
      "           3       1.00      1.00      1.00      4807\n",
      "\n",
      "    accuracy                           0.88     19386\n",
      "   macro avg       0.93      0.88      0.89     19386\n",
      "weighted avg       0.92      0.88      0.89     19386\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.21      0.35       210\n",
      "           1       0.71      0.32      0.44      1360\n",
      "           2       0.37      0.84      0.52       782\n",
      "           3       1.00      0.10      0.18        60\n",
      "\n",
      "    accuracy                           0.47      2412\n",
      "   macro avg       0.77      0.37      0.37      2412\n",
      "weighted avg       0.63      0.47      0.45      2412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=10, random_state=42,n_jobs = -1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_val_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f45fc-7513-4e0c-8610-8f0f5e9976f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RL(X_train_rl, y_train_rl,X_val_rl,y_val_rl,X_test_rl,y_test_rl):\n",
    "#     '''\n",
    "#     A Random forest classifier:\n",
    "        \n",
    "#     '''\n",
    "#     clf = RandomForestClassifier(max_depth=10, random_state=42,n_jobs = -1)\n",
    "#     clf.fit(X_train_rl, y_train_rl)\n",
    "#     y_train_pred = clf.predict(X_train_rl)\n",
    "#     print(\"--------Random forest classifier--------\")\n",
    "#     print(\"*****f1-score metrix on training set*****\")\n",
    "#     print(classification_report(y_train_rl, y_train_pred))\n",
    "\n",
    "#     y_val_pred = clf.predict(X_val_rl)\n",
    "#     print(\"*****f1-score metrix on val set*****\")\n",
    "#     print(classification_report(y_val_rl, y_val_pred))\n",
    "    \n",
    "#     y_test_pred = clf.predict(X_test_rl)\n",
    "#     print(\"*****f1-score metrix on test set*****\")\n",
    "#     print(classification_report(y_test_rl, y_test_pred))\n",
    "\n",
    "# RL(X_train, y_train,X_val,y_val,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5273e73-cce2-4af1-95af-2c9396166610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61a864-b537-4ed8-8945-4629dc87749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MLPClassifier(random_state=43, hidden_layer_sizes= (300,200,100,50),max_iter=1000,learning_rate ='adaptive' ).fit(X_train, y_train)\n",
    "\n",
    "# clf = MLPClassifier( hidden_layer_sizes= (300,200,100,50),early_stopping = True,max_iter=1000,learning_rate ='adaptive' ).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_train_pred = clf.predict(X_train)\n",
    "# print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# y_val_pred = clf.predict(X_val)\n",
    "# print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650aad37-10dc-4890-8c73-03b079e1fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MLP(X_train_mlp, y_train_mlp,X_val_mlp,y_val_mlp,X_test_mlp,y_test_mlp):\n",
    "#     '''\n",
    "#     A MLP classifier:\n",
    "        \n",
    "#     '''\n",
    "#     clf = AdaBoostClassifier(n_estimators=100).fit(X_train_mlp, y_train_mlp)\n",
    "\n",
    "#     y_train_pred = clf.predict(X_train_mlp)\n",
    "#     print(\"--------MLP classifier--------\")\n",
    "#     print(\"*****f1-score metrix on training set*****\")\n",
    "#     print(classification_report(y_train_mlp, y_train_pred))\n",
    "\n",
    "#     y_val_pred = clf.predict(X_val_mlp)\n",
    "#     print(\"*****f1-score metrix on val set*****\")\n",
    "#     print(classification_report(y_val_mlp, y_val_pred))\n",
    "    \n",
    "#     y_test_pred = clf.predict(X_test_mlp)\n",
    "#     print(\"*****f1-score metrix on test set*****\")\n",
    "#     print(classification_report(y_test_mlp, y_test_pred))\n",
    "\n",
    "# RL(X_train, y_train,X_val,y_val,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d271f-e822-4731-b3cc-973803395b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "\n",
    "# y_train_pred = clf.predict(X_train)\n",
    "# print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# y_val_pred = clf.predict(X_val)\n",
    "# print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30aba4a-c72b-4130-af74-24b582a8b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf1 = LogisticRegression(random_state=1)\n",
    "# clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "# clf3 = GaussianNB()\n",
    "# clf4 = AdaBoostClassifier(n_estimators=100)\n",
    "# clf5 = MLPClassifier(random_state=43, max_iter=1000)\n",
    "\n",
    "# eclf = VotingClassifier(\n",
    "#     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3),('ada',clf4),('m;p',clf5)],\n",
    "#     voting='hard',n_jobs = -1).fit(X_train, y_train)\n",
    "\n",
    "# # for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "# #     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "# #     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "# y_train_pred = eclf.predict(X_train)\n",
    "# print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# y_val_pred = eclf.predict(X_val)\n",
    "# print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c0103-3ef6-40b4-8203-91052589a8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2e5d6-ca05-4bd6-a43c-562340f6de69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85c43f9f-c87e-46e0-85f9-24b429d0fe51",
   "metadata": {},
   "source": [
    "### 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc2110-97cc-46fa-9504-b8557ba23751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba8562-a03c-4680-b3bf-2bb93cbbda8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3\n",
      "1    1\n",
      "2    2\n",
      "3    1\n",
      "4    2\n",
      "5    1\n",
      "6    2\n",
      "7    2\n",
      "8    2\n",
      "9    2\n",
      "Name: encoded_label, dtype: int8\n",
      "0    [-0.5068612926736724, -0.5086480876407745, -0....\n",
      "1    [-0.48884841322843875, -0.48713278449435243, -...\n",
      "2    [-0.4992578113244744, -0.4991796862007349, -0....\n",
      "3    [-0.5087331682535241, -0.5107641376148089, -0....\n",
      "4    [-0.4960635803336355, -0.49520472513370145, -0...\n",
      "5    [-0.482258716565533, -0.48220148661897017, -0....\n",
      "6    [-0.5025922829944869, -0.5026462888902054, -0....\n",
      "7    [-0.49699048964593323, -0.4967829372077217, -0...\n",
      "8    [-0.49128891162650795, -0.49092289950997464, -...\n",
      "9    [-0.5042545387709347, -0.5062276582009335, -0....\n",
      "Name: preprocessed_data_wo_fft, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(y_train_pd.head())\n",
    "# print(X_train_pd.head())\n",
    "# X_train_pd.head()\n",
    "\n",
    "\n",
    "print(y_train_pd.head(10))\n",
    "y_train[:10]\n",
    "\n",
    "\n",
    "print(X_train_pd.head(10))\n",
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93550594-6242-4c21-9f83-a728b66ebaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 9000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ecg_Dataset(Dataset):\n",
    "    def __init__(self, X_pd,y_pd):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_pd = X_pd\n",
    "        self.y_pd = y_pd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X_pd.iloc[idx], self.y_pd.iloc[idx]\n",
    "\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd,y_train_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=1000)\n",
    "next(iter(ecg_train_dataloader))[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14777c0-ced7-4293-9e1e-1f7f43431f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74c92b-40cb-4ff4-949e-29c07a462153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(pl.LightningModule):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             num_inputs,\n",
    "#             num_hidden_1,\n",
    "#             num_hidden_2,\n",
    "#             num_hidden_3,\n",
    "#             num_hidden_4,\n",
    "#             num_outputs,\n",
    "#             dataloaders,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.linear1 = nn.Linear(num_inputs, num_hidden_1)\n",
    "#         self.ac1 = nn.ReLU()\n",
    "\n",
    "#         self.linear2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
    "#         self.ac2 = nn.ReLU()\n",
    "\n",
    "#         self.linear3 = nn.Linear(num_hidden_2, num_hidden_3)\n",
    "#         self.ac3 = nn.ReLU()\n",
    "\n",
    "#         self.linear4 = nn.Linear(num_hidden_3, num_hidden_4)\n",
    "#         self.ac4 = nn.ReLU()\n",
    "\n",
    "#         self.out = nn.Linear(num_hidden_4, num_outputs)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.train_accuracy = torchmetrics.F1Score()\n",
    "\n",
    "#         self.val_accuracy = torchmetrics.F1Score()\n",
    "#         self.test_accuracy = torchmetrics.F1Score()\n",
    "#         self.dataloaders = dataloaders\n",
    "\n",
    "#     def forward(self, inputs, labels=None):\n",
    "\n",
    "#         inputs = inputs.float()\n",
    "\n",
    "#         outputs = self.linear1(inputs)\n",
    "#         outputs = self.ac1(outputs)\n",
    "\n",
    "#         outputs = self.linear2(outputs)\n",
    "#         outputs = self.ac2(outputs)\n",
    "\n",
    "#         outputs = self.linear3(outputs)\n",
    "#         outputs = self.ac3(outputs)\n",
    "\n",
    "#         outputs = self.linear4(outputs)\n",
    "#         outputs = self.ac4(outputs)\n",
    "\n",
    "#         outputs = self.out(outputs)\n",
    "\n",
    "#         outputs = self.softmax(outputs)\n",
    "\n",
    "#         return outputs\n",
    "#     def predict_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch\n",
    "\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs)\n",
    "\n",
    "#         # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "#         return preds\n",
    "\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "#         # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "#         return self.dataloaders[0]\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "#         # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "#         return self.dataloaders[1]\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "#         # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "#         return self.dataloaders[2]\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch\n",
    "\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs)\n",
    "\n",
    "#         # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "#         # print(preds)\n",
    "#         self.train_accuracy(preds, labels)\n",
    "\n",
    "#         # print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "#         loss = self.criterion(outputs, labels)\n",
    "#         # print(\"trainingh Loss:\",loss)\n",
    "\n",
    "#         self.log(\"train_loss\", loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch\n",
    "#         # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs)\n",
    "#         # outputs = torch.argmax(outputs, dim=1)\n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "#         self.val_accuracy(preds, labels)\n",
    "#         # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "#         # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "#         loss = self.criterion(outputs, labels)\n",
    "#         # print(\"VAL Loss:\",loss)\n",
    "\n",
    "#         self.log(\"val_loss\", loss)\n",
    "\n",
    "#     def validation_epoch_end(self, outs):\n",
    "#         print(\"total val acc:\", self.val_accuracy.compute())\n",
    "#         print(\"*\" * 25)\n",
    "\n",
    "#     def training_epoch_end(self, outs):\n",
    "#         # print(   compute_epoch_loss_from_outputs(outs))\n",
    "#         print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         decayRate = 0.96\n",
    "#         optimizer = Adam(self.parameters(), lr=LR)\n",
    "#         scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "#         return [optimizer], [scheduler]\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch\n",
    "#         # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs)\n",
    "#         # outputs = torch.argmax(outputs, dim=1)\n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "#         # print(preds)\n",
    "\n",
    "#         self.test_accuracy(preds, labels)\n",
    "\n",
    "#     def test_epoch_end(self, outs):\n",
    "#         # print(   compute_epoch_loss_from_outputs(outs))\n",
    "#         print(\"total test acc:\", self.test_accuracy.compute())\n",
    "\n",
    "# ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "# ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "# ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "# ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# model = MyModel(4501,3000,2000, 1000, 500, 4, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=50, gpus=0)  #\n",
    "\n",
    "#     # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "# trainer.fit(model)\n",
    "# # torch.save(model, \"tmp/saved_model\")\n",
    "# trainer.validate(model)\n",
    "# trainer.test(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c447d83-bbc6-45d0-a677-20d492c20748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428eafc-f914-4708-8daa-094278dba5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class _SepConv1d(nn.Module):\n",
    "#     \"\"\"A simple separable convolution implementation.\n",
    "\n",
    "#     The separable convlution is a method to reduce number of the parameters\n",
    "#     in the deep learning network for slight decrease in predictions quality.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, ni, no, kernel, stride, pad):\n",
    "#         super().__init__()\n",
    "#         self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "#         self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.pointwise(self.depthwise(x))\n",
    "\n",
    "\n",
    "# class SepConv1d(nn.Module):\n",
    "#     \"\"\"Implementes a 1-d convolution with 'batteries included'.\n",
    "\n",
    "#     The module adds (optionally) activation function and dropout layers right after\n",
    "#     a separable convolution layer.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         ni,\n",
    "#         no,\n",
    "#         kernel,\n",
    "#         stride,\n",
    "#         pad,\n",
    "#         drop=None,\n",
    "#         activ=lambda: nn.ReLU(inplace=True),\n",
    "#     ):\n",
    "\n",
    "#         super().__init__()\n",
    "#         assert drop is None or (0.0 < drop < 1.0)\n",
    "#         layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n",
    "#         if activ:\n",
    "#             layers.append(activ())\n",
    "#         if drop is not None:\n",
    "#             layers.append(nn.Dropout(drop))\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "\n",
    "\n",
    "# class Flatten(nn.Module):\n",
    "#     \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "#     def __init__(self, keep_batch_dim=True):\n",
    "#         super().__init__()\n",
    "#         self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.keep_batch_dim:\n",
    "#             return x.view(x.size(0), -1)\n",
    "#         return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36f780-7f02-44c1-a45d-c6248db4c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ecg_Dataset_v2(Dataset):\n",
    "    def __init__(self, X_pd,X_wo_fft_pd,y_pd):\n",
    "        \"\"\"\n",
    "        Return Data with fft and without fft\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_pd = X_pd\n",
    "        self.X_wo_fft_pd = X_wo_fft_pd\n",
    "        self.y_pd = y_pd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X_pd.iloc[idx],self.X_wo_fft_pd.iloc[idx], self.y_pd.iloc[idx]\n",
    "\n",
    "    \n",
    "# meta_precessed_pd = preprocess(meta=meta_pd,func_list=[ecg_len_norm,ecg_fourier,ecg_norm,ecg_stand])\n",
    "\n",
    "# train_pd,  test_pd = train_test_split(meta_precessed_pd, test_size=0.3, random_state=42,stratify =meta_precessed_pd['encoded_label'] )\n",
    "# # print(train_pd.head())\n",
    "# train_pd_ros = data_ros(train_pd,cate_counts_dic )\n",
    "\n",
    "X_train_pd = train_pd_ros['preprocessed_data']\n",
    "y_train_pd = train_pd_ros['encoded_label']\n",
    "X_train_wo_fft_pd = train_pd_ros['preprocessed_data_wo_fft']\n",
    "\n",
    "X_val_pd, X_test_pd, y_val_pd, y_test_pd = train_test_split(test_pd[['preprocessed_data','preprocessed_data_wo_fft']], test_pd['encoded_label'], test_size=0.33, random_state=42,stratify = test_pd['encoded_label'])\n",
    "\n",
    "\n",
    "X_val_pd.reset_index(inplace=True)\n",
    "X_val_wo_fft_pd = X_val_pd['preprocessed_data_wo_fft']\n",
    "X_val_pd = X_val_pd['preprocessed_data']\n",
    "\n",
    "X_test_pd.reset_index(inplace=True)\n",
    "X_test_wo_fft_pd = X_test_pd['preprocessed_data_wo_fft']\n",
    "X_test_pd = X_test_pd['preprocessed_data']\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "ecg_train_dataset = ecg_Dataset_v2(X_train_pd,X_train_wo_fft_pd,y_train_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_val_dataset = ecg_Dataset_v2(X_val_pd,X_val_wo_fft_pd, y_val_pd)\n",
    "ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_test_dataset = ecg_Dataset_v2(X_test_pd, X_test_wo_fft_pd ,y_test_pd)\n",
    "ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c3bbb-335c-45f0-bbac-7600355edfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel_1dCNN(pl.LightningModule):\n",
    "#     def __init__(self, cin_raw,cin_fft, num_outputs, drop,  dataloaders):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.seq_fft= nn.Sequential(\n",
    "#             SepConv1d(cin_raw, 32, 8, 2, 3, drop=drop), # cin 24\n",
    "#             SepConv1d(32, 64, 8, 4, 2, drop=drop),\n",
    "#             SepConv1d(64, 128, 8, 4, 2, drop=drop),\n",
    "#             SepConv1d(128, 256, 8, 4, 2),\n",
    "#             Flatten(),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(8960, 3000), # 18176\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(3000, 1500),\n",
    "#             nn.ReLU(inplace=True),\n",
    "        \n",
    "#         )\n",
    "\n",
    "#         self.seq_raw = nn.Sequential(\n",
    "#             SepConv1d(cin_fft,  32, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(    32,  64, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(    64, 128, 8, 4, 4, drop=drop),\n",
    "#             SepConv1d(   128, 128, 8, 4, 4, drop=drop),\n",
    "#             SepConv1d(   128, 256, 8, 2, 3),\n",
    "#             Flatten(),\n",
    "#             nn.Dropout(drop), nn.Linear(17920, 3000), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop), nn.Linear( 3000, 1500), nn.ReLU(inplace=True),)\n",
    "        \n",
    "#         # self.out = nn.Linear(64, num_outputs)\n",
    "        \n",
    "#         self.out = nn.Sequential(\n",
    "#             nn.Linear(3000, 2000), nn.ReLU(inplace=True), \n",
    "#             nn.Linear(2000, 1000), nn.ReLU(inplace=True),\n",
    "#             nn.Linear(1000, 500), nn.ReLU(inplace=True),\n",
    "#             nn.Linear(500, num_outputs), nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.train_accuracy = torchmetrics.F1Score()\n",
    "\n",
    "#         self.val_accuracy = torchmetrics.F1Score()\n",
    "        \n",
    "#         self.test_accuracy = torchmetrics.F1Score()\n",
    "#         self.dataloaders = dataloaders\n",
    "\n",
    "#     def forward(self, inputs,inputs_wo_fft, labels=None):\n",
    "\n",
    "#         inputs = inputs.float()\n",
    "#         inputs_wo_fft = inputs_wo_fft.float()\n",
    "#         # print(inputs.shape)\n",
    "#         outputs_fft = self.seq_fft(inputs)\n",
    "#         outputs_raw = self.seq_raw(inputs_wo_fft)\n",
    "        \n",
    "#         # outputs_fft = self.seq_fft(inputs)\n",
    "#         outputs = torch.cat([outputs_raw,outputs_fft],dim=1)\n",
    "\n",
    "    \n",
    "#         return outputs\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "#         # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "#         return self.dataloaders[0]\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "#         # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "#         return self.dataloaders[1]\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "#         # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "#         return self.dataloaders[2]\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "\n",
    "#         inputs,inputs_wo_fft, labels = batch\n",
    "#         inputs = inputs\n",
    "#         inputs_wo_fft = inputs_wo_fft\n",
    "        \n",
    "#         inputs = inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "#         inputs_wo_fft  = inputs_wo_fft.reshape(inputs_wo_fft.shape[0],1,inputs_wo_fft.shape[1])\n",
    "#         #print(inputs.shape)\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs,inputs_wo_fft)\n",
    "#         preds = F.log_softmax(outputs, dim=1).argmax(dim=1)\n",
    "#         #preds = torch.argmax(outputs, dim=1)\n",
    "#         #print(preds.shape)\n",
    "\n",
    "        \n",
    "#         self.train_accuracy(preds, labels)\n",
    "        \n",
    "\n",
    "#         #print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "#         loss = self.criterion(outputs, labels)\n",
    "\n",
    "#         self.log(\"train_loss\", loss)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#         # loss, outputs = self(inputs, labels)\n",
    "#         # return {\"loss\": loss}\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "\n",
    "#         inputs,inputs_wo_fft, labels = batch\n",
    "#         inputs = inputs\n",
    "#         inputs_wo_fft = inputs_wo_fft\n",
    "#         inputs = inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        \n",
    "#         inputs_wo_fft  = inputs_wo_fft.reshape(inputs_wo_fft.shape[0],1,inputs_wo_fft.shape[1])\n",
    "#         #print(inputs.shape)\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs,inputs_wo_fft)\n",
    "#         preds = F.log_softmax(outputs, dim=1).argmax(dim=1)\n",
    "#         #print(preds.shape)\n",
    "\n",
    "        \n",
    "#         self.val_accuracy(preds, labels)\n",
    "      \n",
    "        \n",
    "\n",
    "#         # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "#         # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "#         loss = self.criterion(outputs, labels)\n",
    "\n",
    "#         self.log(\"val_loss\", loss)\n",
    "\n",
    "#     def validation_epoch_end(self, outs):\n",
    "#         print(\"total val acc:\", self.val_accuracy.compute())\n",
    "#         # print(self.val_f1_matrix)\n",
    "#         #print(\"*\" * 25)\n",
    "\n",
    "#     def training_epoch_end(self, outs):\n",
    "#         #pass\n",
    "#         print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "\n",
    "#         optimizer = Adam(self.parameters(),lr=0.0001)\n",
    "#         return optimizer\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         inputs,inputs_wo_fft, labels = batch\n",
    "#         inputs = inputs\n",
    "#         inputs_wo_fft = inputs_wo_fft\n",
    "#         inputs = inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        \n",
    "#         inputs_wo_fft  = inputs_wo_fft.reshape(inputs_wo_fft.shape[0],1,inputs_wo_fft.shape[1])\n",
    "#         #print(inputs.shape)\n",
    "#         labels = labels.long()\n",
    "\n",
    "#         outputs = self(inputs,inputs_wo_fft)\n",
    "#         preds = F.log_softmax(outputs, dim=1).argmax(dim=1)\n",
    "#         #print(preds.shape)\n",
    "\n",
    "        \n",
    "#         self.test_accuracy(preds, labels)\n",
    "#     def test_epoch_end(self, outs):\n",
    "#         # print(   compute_epoch_loss_from_outputs(outs))\n",
    "#         print(\"total test acc:\", self.test_accuracy.compute())  \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = MyModel_1dCNN(1,1, 4, drop=0.3, dataloaders=[ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "#     trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=200, gpus=0,)\n",
    "#     trainer.fit(model)\n",
    "    \n",
    "#     trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c212503-105d-4f06-a641-da065c7016e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop = 0.1\n",
    "# seq_raw=  nn.Sequential(\n",
    "#             SepConv1d(1,  32, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(    32,  64, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(    64, 128, 8, 4, 4, drop=drop),\n",
    "#             SepConv1d(   128, 128, 8, 4, 4, drop=drop),\n",
    "#             SepConv1d(   128, 256, 8, 2, 3),\n",
    "# Flatten(),)\n",
    "# a= torch.rand([1,1,9000])\n",
    "# b= seq_raw(a)\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba727b",
   "metadata": {},
   "source": [
    "## 1d cnn from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac2a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cin_fft = 1\n",
    "k_size = 5\n",
    "seq_fft = nn.Sequential(\n",
    "    nn.Conv1d(cin_fft,32,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,32,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,64,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,64,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,128,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,128,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,256,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(256,256,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Conv1d(256,512,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(512,512,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    # nn.Linear(2048,128),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.Dropout(0.5),\n",
    "    # nn.Linear(128,32),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.Linear(32,4),\n",
    "    # nn.Softmax(dim=1)\n",
    "        \n",
    "        )\n",
    "a = torch.rand([10,1,9000])\n",
    "b = seq_fft(a)\n",
    "b.shape\n",
    "b= b.reshape(b.shape[0],b.shape[1],1)\n",
    "b.shape #(10,2048,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead09416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs =10\n",
    "# in_fea = 1\n",
    "# len = 9000\n",
    "# hs= 1\n",
    "# num_layers = 2\n",
    "# c = torch.rand([batch_size,len,in_fea])\n",
    "\n",
    "# rnn = nn.LSTM(in_fea, hs, num_layers,batch_first = True,bidirectional = True)\n",
    "# input = c\n",
    "# h0 = torch.randn(2*num_layers,batch_size, hs)\n",
    "# c0 = torch.randn(2*num_layers, batch_size, hs)\n",
    "# output, (hn, cn) = rnn(input, (h0, c0))\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb4c712a-5441-4235-8a54-1292384cda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ecg_Dataset(Dataset):\n",
    "    def __init__(self, X_pd,y_pd):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_pd = X_pd\n",
    "        self.y_pd = y_pd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X_pd.iloc[idx], self.y_pd.iloc[idx]\n",
    "\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cin,\n",
    "            cout,\n",
    "\n",
    "            dataloaders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cin= cin,\n",
    "        self.example_input_array = torch.rand(1,1,9000)\n",
    "        self.cout=cout\n",
    "\n",
    "        \n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "\n",
    "        self.val_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.test_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.dataloaders = dataloaders\n",
    "    \n",
    "        self.feature_extractor =  nn.Sequential(\n",
    "    nn.Conv1d(1,32,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,32,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,64,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,64,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,128,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,128,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,256,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(256,256,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Conv1d(256,512,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(512,512,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Flatten(),\n",
    "        )\n",
    "    \n",
    "        self.classifier =  nn.Sequential(\n",
    "            nn.Linear(2048,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128,32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(32,self.cout),\n",
    "    nn.Softmax(dim=1))\n",
    "\n",
    "    def get_feature(self, inputs):\n",
    "\n",
    "        features = self.feature_extractor(inputs)\n",
    "        # print('feature shape:',features.shape)\n",
    "        return features \n",
    "\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        #outputs = self.seq_fft(inputs)\n",
    "\n",
    "        outputs = self.feature_extractor(inputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "        # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[0]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "        # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[1]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "        # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "        return self.dataloaders[2]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        #print(inputs.shape)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # print(preds)\n",
    "        self.train_accuracy(preds, labels)\n",
    "\n",
    "        # print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"trainingh Loss:\",loss)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "        # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"VAL Loss:\",loss)\n",
    "        #self.val_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        #print(preds)\n",
    "        self.test_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        # print(preds)\n",
    "\n",
    "        self.test_accuracy(preds, labels)\n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"val_f1\",self.val_accuracy.compute())\n",
    "        print(\"total val acc:\", self.val_accuracy.compute())\n",
    "        #print(self.val_f1_matrix)\n",
    "        print(\"*\" * 25)\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        self.log(\"training_f1\",self.train_accuracy.compute())\n",
    "        print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        print(\"total test acc:\", self.test_accuracy.compute())\n",
    "        print(self.test_f1_matrix)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decayRate = 0.96\n",
    "        optimizer = Adam(self.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    \n",
    "LR = 1e-4  \n",
    "BATCH_SIZE =30\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e3bc2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params | In sizes     | Out sizes\n",
      "----------------------------------------------------------------------------------\n",
      "0 | criterion         | CrossEntropyLoss | 0      | ?            | ?        \n",
      "1 | train_accuracy    | F1Score          | 0      | ?            | ?        \n",
      "2 | val_accuracy      | F1Score          | 0      | ?            | ?        \n",
      "3 | test_accuracy     | F1Score          | 0      | ?            | ?        \n",
      "4 | feature_extractor | Sequential       | 2.6 M  | [1, 1, 9000] | [1, 2048]\n",
      "5 | classifier        | Sequential       | 266 K  | [1, 2048]    | [1, 4]   \n",
      "----------------------------------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.540    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00,  2.20it/s]total val acc: tensor(0.0182, device='cuda:0')\n",
      "*************************\n",
      "Epoch 0: 100%|| 728/728 [00:18<00:00, 40.38it/s, loss=1.16, v_num=7]total val acc: tensor(0.4810, device='cuda:0')\n",
      "*************************\n",
      "Epoch 0: 100%|| 728/728 [00:18<00:00, 40.37it/s, loss=1.16, v_num=7]total training acc: tensor(0.4335, device='cuda:0')\n",
      "Epoch 0: 100%|| 728/728 [00:18<00:00, 40.19it/s, loss=1.16, v_num=7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 81/81 [00:00<00:00, 101.71it/s]total val acc: tensor(0.4846, device='cuda:0')\n",
      "*************************\n",
      "Validation DataLoader 0: 100%|| 81/81 [00:00<00:00, 101.63it/s]\n",
      "\n",
      "     Validate metric           DataLoader 0\n",
      "\n",
      "         val_f1             0.4845898747444153\n",
      "        val_loss             1.228154182434082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 2/2 [00:01<00:00,  1.57it/s]total test acc: tensor(0.4741, device='cuda:0')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.50      0.31        20\n",
      "           1       0.63      0.70      0.67       105\n",
      "           2       0.50      0.05      0.09        60\n",
      "           3       0.14      1.00      0.25         3\n",
      "\n",
      "    accuracy                           0.48       188\n",
      "   macro avg       0.38      0.56      0.33       188\n",
      "weighted avg       0.54      0.48      0.44       188\n",
      "\n",
      "Testing DataLoader 0: 100%|| 2/2 [00:01<00:00,  1.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\",log_graph=True,name=\"BS\")\n",
    "model = MyModel(1,4, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "# print(model)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1, gpus=0,logger= logger)  #\n",
    "\n",
    "    # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "trainer.fit(model)\n",
    "# torch.save(model, \"tmp/saved_model\")\n",
    "trainer.validate(model)\n",
    "trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP = [10,50,100,150,200,300]\n",
    "\n",
    "# # EP = [1,2,3]\n",
    "# for ep in EP:\n",
    "#     logger = TensorBoardLogger(\"tb_logs\",log_graph=True,name=f\"BS_{ep}\")\n",
    "#     model = MyModel(1,4, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "#     # print(model)\n",
    "#     trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=ep, gpus=0,logger= logger)  #\n",
    "\n",
    "#         # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "#     trainer.fit(model)\n",
    "#     # torch.save(model, \"tmp/saved_model\")\n",
    "#     trainer.validate(model)\n",
    "#     trainer.test(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98fafb",
   "metadata": {},
   "source": [
    "### prepare QRS data for LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e406f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "from sklearn.preprocessing import Normalizer\n",
    "fs = 300\n",
    "\n",
    "transformer = Normalizer()\n",
    "\n",
    "meta_precessed_pd['clean'] = meta_precessed_pd['preprocessed_data'].apply(nk.ecg_clean,sampling_rate = fs)\n",
    "## get mean, std, min, max of cleaned data\n",
    "meta_precessed_pd['clean_mean'] = meta_precessed_pd['clean'].apply(np.mean)\n",
    "meta_precessed_pd['clean_std'] = meta_precessed_pd['clean'].apply(np.std)\n",
    "meta_precessed_pd['clean_max'] = meta_precessed_pd['clean'].apply(np.max)\n",
    "meta_precessed_pd['clean_min'] = meta_precessed_pd['clean'].apply(np.min)\n",
    "\n",
    "## get the distance between max and min\n",
    "meta_precessed_pd['clean_dist'] =meta_precessed_pd['clean_max']-meta_precessed_pd['clean_min']\n",
    "meta_precessed_pd['clean_dist'] = meta_precessed_pd['clean_dist'].apply(np.abs)\n",
    "meta_precessed_pd.head()\n",
    "\n",
    "\n",
    "def get_segment(ecg_np):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        ecg_np: np.array, cleaned ecg data, (9000)\n",
    "    \n",
    "\n",
    "    Return:\n",
    "        Segment: np.array, \n",
    "    '''\n",
    "    ecg_np = transformer.fit_transform(ecg_np.reshape(1, -1)).reshape(9000)\n",
    "    _, rpeaks = nk.ecg_peaks(ecg_np.reshape(9000), sampling_rate=300, correct_artifacts=True,method='promac')\n",
    "    # elgendi2010,\n",
    "    epochs = nk.ecg_segment(ecg_np, rpeaks=rpeaks[\"ECG_R_Peaks\"], sampling_rate=300)\n",
    "    seg_len = len(epochs['1']['Signal'].values)\n",
    "    seg = np.zeros([1,seg_len])\n",
    "    for i in epochs.values():\n",
    "        if True in np.isnan(i['Signal'].values):\n",
    "            pass\n",
    "        else:\n",
    "            seg += transformer.fit_transform(i['Signal'].values.reshape(1, -1)) \n",
    "\n",
    "\n",
    "    seg = transformer.fit_transform(seg.reshape(1,-1))\n",
    "    return seg.reshape(seg_len)\n",
    "\n",
    "meta_precessed_pd['hb_seg'] = meta_precessed_pd['clean'].map(get_segment)\n",
    "meta_precessed_pd['seg_len'] = meta_precessed_pd['hb_seg'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8360c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_precessed_pd['seg_len'] = meta_precessed_pd['hb_seg'].map(len)\n",
    "meta_precessed_pd['hb_seg'] = meta_precessed_pd['hb_seg'].apply(signal.resample,num=300)\n",
    "meta_precessed_pd.head()\n",
    "\n",
    "meta_precessed_pd.to_csv('meta_precessed_pd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d586f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'path', 'label', 'encoded_label', 'data', 'mean',\n",
       "       'std', 'length', 'to_path', 'preprocessed_data',\n",
       "       'preprocessed_data_wo_fft', 'clean', 'clean_mean', 'clean_std',\n",
       "       'clean_max', 'clean_min', 'clean_dist', 'hb_seg', 'seg_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "meta_precessed_pd_loaded = pd.read_csv('meta_precessed_pd.csv',delimiter=\",\")\n",
    "meta_precessed_pd_loaded.columns\n",
    "# meta_pd.columns\n",
    "# meta_precessed_pd = meta_precessed_pd_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b1ec3",
   "metadata": {},
   "source": [
    "### CNN + LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c918af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 6.0, 'N': 1.0, 'O': 2.0, '~': 23.0}\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# X_train_pd.head()\n",
    "# meta_precessed_pd.columns\n",
    "# meta_precessed_pd['hb_seg'] = [ np.random.rand(300) for i in range(len(meta_precessed_pd))]\n",
    "# len(meta_precessed_pd)\n",
    "# meta_precessed_pd['hb_seg']\n",
    "\n",
    "train_pd,  test_pd = train_test_split(meta_precessed_pd, test_size=0.3, random_state=42,stratify =meta_precessed_pd['encoded_label'] )\n",
    "train_pd_ros = data_ros(train_pd,cate_counts_dic )\n",
    "\n",
    "X_train_pd = train_pd_ros[['preprocessed_data_wo_fft','hb_seg']]\n",
    "X_train_hb_pd = X_train_pd['hb_seg']\n",
    "X_train_pd = X_train_pd['preprocessed_data_wo_fft']\n",
    "\n",
    "y_train_pd = train_pd_ros['encoded_label']\n",
    "X_val_pd, X_test_pd, y_val_pd, y_test_pd = train_test_split(test_pd[['preprocessed_data_wo_fft','hb_seg']], test_pd['encoded_label'], test_size=0.33, random_state=42,stratify = test_pd['encoded_label'])\n",
    "\n",
    "# X_train_hb_pd\n",
    "# X_train_pd\n",
    "X_val_hb_pd= X_val_pd['hb_seg']\n",
    "X_val_pd= X_val_pd['preprocessed_data_wo_fft']\n",
    "\n",
    "X_test_hb_pd= X_test_pd['hb_seg']\n",
    "X_test_pd= X_test_pd['preprocessed_data_wo_fft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "722a564d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f56368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49935428, -0.49911214, -0.49895071, ..., -0.50032286,\n",
       "       -0.50104929, -0.50169501])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta_precessed_pd.iloc[3]['hb_seg'])\n",
    "X_train_pd.iloc[1]\n",
    "# X_train_pd.head()\n",
    "# type(meta_pd['data'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ecg_Dataset(Dataset):\n",
    "    def __init__(self, X_pd,y_pd,hb_pd):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_pd = X_pd\n",
    "        self.y_pd = y_pd\n",
    "        self.hb_pd = hb_pd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X_pd.iloc[idx], self.y_pd.iloc[idx], self.hb_pd.iloc[idx]\n",
    "\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd,y_train_pd,X_train_hb_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=1)\n",
    "# print(next(iter(ecg_train_dataloader)))\n",
    "for i in next(iter(ecg_train_dataloader)):\n",
    "    \n",
    "    print(type(i))\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc1285e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cin,\n",
    "            cout,\n",
    "            lstm_in,\n",
    "            lstm_size,\n",
    "            lstm_layer_num,\n",
    "\n",
    "            dataloaders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cin= cin,\n",
    "        #self.example_input_array = torch.rand(1,1,9000)\n",
    "        self.cout=cout\n",
    "\n",
    "        self.lstm_in = lstm_in,\n",
    "        self.lstm_size = lstm_size,\n",
    "        self.lstm_layer_num = lstm_layer_num,\n",
    "\n",
    "        \n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "\n",
    "        self.val_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.test_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.dataloaders = dataloaders\n",
    "    \n",
    "        self.feature_extractor =  nn.Sequential(\n",
    "    nn.Conv1d(1,32,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,32,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,64,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,64,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,128,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,128,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,256,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(256,256,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Conv1d(256,512,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(512,512,5),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Flatten(),\n",
    "        )\n",
    "        self.bilstm = nn.Sequential(\n",
    "            #self.lstm_in, self.lstm_size, self.lstm_layer_num,\n",
    "        nn.LSTM(lstm_in, lstm_size, lstm_layer_num,batch_first = True,bidirectional = True),\n",
    "        extract_tensor(),\n",
    "        nn.Flatten()  )\n",
    "        self.classifier =  nn.Sequential(\n",
    "            nn.Linear(5048,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128,32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32,self.cout),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def get_feature(self, inputs):\n",
    "\n",
    "        features = self.feature_extractor(inputs)\n",
    "        # print('feature shape:',features.shape)\n",
    "        return features \n",
    "\n",
    "\n",
    "    def forward(self, inputs,lstm_inputs, labels=None):\n",
    "\n",
    "        inputs = inputs.float()\n",
    "        lstm_inputs = lstm_inputs.float()\n",
    "        #outputs = self.seq_fft(inputs)\n",
    "\n",
    "        outputs = self.feature_extractor(inputs)\n",
    "        # print(lstm_inputs.shape)\n",
    "        lstm_outputs = self.bilstm(lstm_inputs)\n",
    "        # print(outputs.shape) #[bs,2048]\n",
    "        # print(lstm_outputs.shape) # [bs,3000]\n",
    "        outputs = torch.cat((outputs,lstm_outputs),dim=1)\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels,lstm_inputs = batch\n",
    "        lstm_inputs = lstm_inputs.reshape(lstm_inputs.shape[0],lstm_inputs.shape[1],1)\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs,lstm_inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "        # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[0]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "        # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[1]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "        # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "        return self.dataloaders[2]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels,lstm_inputs = batch\n",
    "        # print(lstm_inputs.shape) # [batch_size,300]\n",
    "        lstm_inputs = lstm_inputs.reshape(lstm_inputs.shape[0],lstm_inputs.shape[1],1)\n",
    "        #print(lstm_inputs.shape) # [batch_size,300,1]\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        #print(inputs.shape)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs,lstm_inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # print(preds)\n",
    "        self.train_accuracy(preds, labels)\n",
    "\n",
    "        # print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"trainingh Loss:\",loss)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels,lstm_inputs = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "\n",
    "        lstm_inputs = lstm_inputs.reshape(lstm_inputs.shape[0],lstm_inputs.shape[1],1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs,lstm_inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "        # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"VAL Loss:\",loss)\n",
    "        #self.val_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels,lstm_inputs = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        lstm_inputs = lstm_inputs.reshape(lstm_inputs.shape[0],lstm_inputs.shape[1],1)\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs,lstm_inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        #print(preds)\n",
    "        self.test_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        # print(preds)\n",
    "\n",
    "        self.test_accuracy(preds, labels)\n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"val_f1\",self.val_accuracy.compute())\n",
    "        print(\"total val acc:\", self.val_accuracy.compute())\n",
    "        #print(self.val_f1_matrix)\n",
    "        print(\"*\" * 25)\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        self.log(\"training_f1\",self.train_accuracy.compute())\n",
    "        print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        print(\"total test acc:\", self.test_accuracy.compute())\n",
    "        print(self.test_f1_matrix)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decayRate = 0.96\n",
    "        optimizer = Adam(self.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[1],1)\n",
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        out,hs_cs = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        return out\n",
    "\n",
    "LR = 1e-4  \n",
    "BATCH_SIZE =30\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd,X_train_hb_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd,X_val_hb_pd)\n",
    "ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd,X_test_hb_pd)\n",
    "ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=2000, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4766023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | criterion         | CrossEntropyLoss | 0     \n",
      "1 | train_accuracy    | F1Score          | 0     \n",
      "2 | val_accuracy      | F1Score          | 0     \n",
      "3 | test_accuracy     | F1Score          | 0     \n",
      "4 | feature_extractor | Sequential       | 2.6 M \n",
      "5 | bilstm            | Sequential       | 1.0 K \n",
      "6 | classifier        | Sequential       | 650 K \n",
      "-------------------------------------------------------\n",
      "3.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 M     Total params\n",
      "13.080    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00,  5.04it/s]total val acc: tensor(0.0005, device='cuda:0')\n",
      "*************************\n",
      "Epoch 0: 100%|| 728/728 [00:30<00:00, 23.67it/s, loss=1.14, v_num=25]total val acc: tensor(0.4839, device='cuda:0')\n",
      "*************************\n",
      "Epoch 0: 100%|| 728/728 [00:30<00:00, 23.67it/s, loss=1.14, v_num=25]total training acc: tensor(0.4511, device='cuda:0')\n",
      "Epoch 1: 100%|| 728/728 [00:30<00:00, 23.85it/s, loss=1.12, v_num=25]total val acc: tensor(0.5502, device='cuda:0')\n",
      "*************************\n",
      "Epoch 1: 100%|| 728/728 [00:30<00:00, 23.85it/s, loss=1.12, v_num=25]total training acc: tensor(0.5318, device='cuda:0')\n",
      "Epoch 2: 100%|| 728/728 [00:30<00:00, 23.94it/s, loss=1.01, v_num=25] total val acc: tensor(0.5562, device='cuda:0')\n",
      "*************************\n",
      "Epoch 2: 100%|| 728/728 [00:30<00:00, 23.94it/s, loss=1.01, v_num=25]total training acc: tensor(0.5824, device='cuda:0')\n",
      "Epoch 3: 100%|| 728/728 [00:30<00:00, 24.00it/s, loss=1.02, v_num=25] total val acc: tensor(0.5841, device='cuda:0')\n",
      "*************************\n",
      "Epoch 3: 100%|| 728/728 [00:30<00:00, 24.00it/s, loss=1.02, v_num=25]total training acc: tensor(0.6171, device='cuda:0')\n",
      "Epoch 4: 100%|| 728/728 [00:30<00:00, 23.96it/s, loss=0.993, v_num=25]total val acc: tensor(0.6149, device='cuda:0')\n",
      "*************************\n",
      "Epoch 4: 100%|| 728/728 [00:30<00:00, 23.96it/s, loss=0.993, v_num=25]total training acc: tensor(0.6430, device='cuda:0')\n",
      "Epoch 5: 100%|| 728/728 [00:30<00:00, 23.94it/s, loss=0.939, v_num=25]total val acc: tensor(0.6351, device='cuda:0')\n",
      "*************************\n",
      "Epoch 5: 100%|| 728/728 [00:30<00:00, 23.94it/s, loss=0.939, v_num=25]total training acc: tensor(0.6637, device='cuda:0')\n",
      "Epoch 6: 100%|| 728/728 [00:30<00:00, 24.06it/s, loss=0.958, v_num=25]total val acc: tensor(0.6480, device='cuda:0')\n",
      "*************************\n",
      "Epoch 6: 100%|| 728/728 [00:30<00:00, 24.06it/s, loss=0.958, v_num=25]total training acc: tensor(0.6795, device='cuda:0')\n",
      "Epoch 7: 100%|| 728/728 [00:30<00:00, 24.04it/s, loss=0.929, v_num=25]total val acc: tensor(0.6628, device='cuda:0')\n",
      "*************************\n",
      "Epoch 7: 100%|| 728/728 [00:30<00:00, 24.04it/s, loss=0.929, v_num=25]total training acc: tensor(0.6927, device='cuda:0')\n",
      "Epoch 8: 100%|| 728/728 [00:30<00:00, 24.02it/s, loss=0.906, v_num=25]total val acc: tensor(0.6753, device='cuda:0')\n",
      "*************************\n",
      "Epoch 8: 100%|| 728/728 [00:30<00:00, 24.02it/s, loss=0.906, v_num=25]total training acc: tensor(0.7041, device='cuda:0')\n",
      "Epoch 9: 100%|| 728/728 [00:30<00:00, 24.03it/s, loss=0.92, v_num=25] total val acc: tensor(0.6845, device='cuda:0')\n",
      "*************************\n",
      "Epoch 9: 100%|| 728/728 [00:30<00:00, 24.02it/s, loss=0.92, v_num=25]total training acc: tensor(0.7140, device='cuda:0')\n",
      "Epoch 9: 100%|| 728/728 [00:30<00:00, 23.95it/s, loss=0.92, v_num=25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 81/81 [00:01<00:00, 52.89it/s]total val acc: tensor(0.6920, device='cuda:0')\n",
      "*************************\n",
      "Validation DataLoader 0: 100%|| 81/81 [00:01<00:00, 52.81it/s]\n",
      "\n",
      "     Validate metric           DataLoader 0\n",
      "\n",
      "         val_f1             0.6920296549797058\n",
      "        val_loss             0.972978413105011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 2/2 [00:00<00:00,  4.88it/s]total test acc: tensor(0.7474, device='cuda:0')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80        18\n",
      "           1       0.83      0.87      0.85       105\n",
      "           2       0.79      0.61      0.69        62\n",
      "           3       0.38      1.00      0.55         3\n",
      "\n",
      "    accuracy                           0.79       188\n",
      "   macro avg       0.68      0.84      0.72       188\n",
      "weighted avg       0.80      0.79      0.79       188\n",
      "\n",
      "Testing DataLoader 0: 100%|| 2/2 [00:00<00:00,  7.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\",log_graph=True,name=\"CNN_LSTM\")\n",
    "model = MyModel(cin=1,cout=4, lstm_in=1,lstm_size=5,lstm_layer_num=2, dataloaders=[ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "# print(model)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=100, gpus=0,logger= logger)  #\n",
    "\n",
    "# trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "trainer.fit(model)\n",
    "# torch.save(model, \"tmp/saved_model\")\n",
    "trainer.validate(model)\n",
    "trainer.test(model)\n",
    "# trainer.predict(model, dataloaders=ecg_test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d167b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting the predictions...\n",
      "Testing DataLoader 0: 100%|| 1/1 [00:01<00:00,  1.42s/it]total test acc: tensor(0.9148, device='cuda:0')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92       104\n",
      "           1       0.94      0.95      0.94       669\n",
      "           2       0.90      0.85      0.88       385\n",
      "           3       0.77      0.90      0.83        30\n",
      "\n",
      "    accuracy                           0.91      1188\n",
      "   macro avg       0.87      0.91      0.89      1188\n",
      "weighted avg       0.92      0.91      0.91      1188\n",
      "\n",
      "Testing DataLoader 0: 100%|| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    saved_model = MyModel(cin=1,cout=4, lstm_in=1,lstm_size=5,lstm_layer_num=2, dataloaders=[])\n",
    "\n",
    "    saved_model.load_state_dict(torch.load('mymodel'))\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices=1, gpus=0)\n",
    "    print(\"getting the predictions...\")\n",
    "    trainer.test(saved_model, dataloaders=ecg_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd42f5",
   "metadata": {},
   "source": [
    "## Get deep feature from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13841a-064c-40e7-b464-77f54b975901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0391,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get deep feature from CNN\n",
    "model = MyModel(1,4, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "\n",
    "model.get_feature(torch.tensor(range(9000)).reshape([1,1,9000]).float())\n",
    "\n",
    "# def get_cnn_feature(ecg_np):\n",
    "#     fea = model.get_feature(torch.from_numpy(ecg_np.reshape([1,1,len(ecg_np)])).float())\n",
    "#     fea = fea.reshape(fea.shape[1]) # shoudl be 9000\n",
    "#     return fea\n",
    "# meta_precessed_pd['preprocessed_data_wo_fft'][0]\n",
    "\n",
    "# # fea1 = get_cnn_feature(model,meta_precessed_pd['preprocessed_data_wo_fft'][0])\n",
    "\n",
    "# meta_precessed_pd['fea_cnn']=meta_precessed_pd['preprocessed_data_wo_fft'].map(get_cnn_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meta_precessed_pd['deep_fea']= range(len(meta_precessed_pd))\n",
    "# fea_cnn = []\n",
    "# for row in tqdm(meta_precessed_pd.iterrows()):\n",
    "#     fea = model.get_feature(torch.tensor(row[1]['preprocessed_data_wo_fft'].reshape([1,1,9000])).float()).cpu()\n",
    "#     fea_cnn.append(fea)\n",
    "#     # # print(type(row[1]['preprocessed_data_wo_fft']))\n",
    "#     # # print(fea)\n",
    "#     # row[1]['deep_fea'] = 1\n",
    "#     # meta_precessed_pd.iloc[1]['deep_fea'] = 1\n",
    "#     #print(fea)\n",
    "\n",
    "# meta_precessed_pd['fea_cnn'] = fea_cnn\n",
    "# # # type(meta_precessed_pd[ meta_precessed_pd['id']=='train_ecg_01607' ])\n",
    "# # # meta_precessed_pd.iloc[1]['id'] = 'a'\n",
    "# # # meta_precessed_pd.iloc[1]['id']\n",
    "# meta_precessed_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2431f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_precessed_pd['deep_fea']= range(len(meta_precessed_pd))\n",
    "# fea_cnn = []\n",
    "# for i in tqdm(range(len(meta_precessed_pd))):\n",
    "#     fea = model.get_feature(torch.tensor(meta_precessed_pd.iloc[i]['preprocessed_data_wo_fft'].reshape([1,1,9000])).float()).cpu()\n",
    "\n",
    "#     #fea = model.get_feature(torch.tensor(row[1]['preprocessed_data_wo_fft'].reshape([1,1,9000])).float()).cpu()\n",
    "#     fea_cnn.append(fea)\n",
    "\n",
    "#     #print(fea)\n",
    "\n",
    "# meta_precessed_pd['fea_cnn'] = fea_cnn\n",
    "# # # type(meta_precessed_pd[ meta_precessed_pd['id']=='train_ecg_01607' ])\n",
    "# # # meta_precessed_pd.iloc[1]['id'] = 'a'\n",
    "# # # meta_precessed_pd.iloc[1]['id']\n",
    "# meta_precessed_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99843bda",
   "metadata": {},
   "source": [
    "### LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b4618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size =10\n",
    "# in_fea = 1\n",
    "# len = 2048\n",
    "# hs= 1\n",
    "# num_layers = 2\n",
    "# c = torch.rand([batch_size,len,in_fea])\n",
    "\n",
    "# rnn = nn.LSTM(in_fea, hs, num_layers,batch_first = True,bidirectional = True)\n",
    "# input = c\n",
    "# h0 = torch.randn(2*num_layers,batch_size, hs)\n",
    "# c0 = torch.randn(2*num_layers, batch_size, hs)\n",
    "# output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "# output.shape\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[1],1)\n",
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        out,hs_cs = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        return out\n",
    "\n",
    "c = torch.rand([10,300,1])\n",
    "bilstm = nn.Sequential(\n",
    "#    Reshape(10,2048,1),\n",
    "   nn.LSTM(1, 5, 2,batch_first = True,bidirectional = True),\n",
    "   extract_tensor(),\n",
    "   nn.Flatten()\n",
    "   \n",
    ")\n",
    "\n",
    "d = bilstm(c)\n",
    "d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2132613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cin,\n",
    "            cout,\n",
    "\n",
    "            dataloaders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cin= cin,\n",
    "        # self.example_input_array = torch.rand(1,1,9000)\n",
    "        self.example_input_array = torch.rand(1,9000,1)\n",
    "        self.cout=cout\n",
    "\n",
    "        \n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "\n",
    "        self.val_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.test_accuracy = torchmetrics.F1Score(num_classes=4, average ='weighted')\n",
    "        self.dataloaders = dataloaders\n",
    "    \n",
    "        self.feature_extractor =  nn.Sequential(\n",
    "            nn.LSTM(1, 2, 2,batch_first = True,bidirectional = True),\n",
    "   extract_tensor(),\n",
    "   nn.Flatten()\n",
    "    # nn.Conv1d(1,32,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.BatchNorm1d(32),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(32,32,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(32,64,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(64,64,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(64,128,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(128,128,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(128,256,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(256,256,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "    # nn.Dropout(0.5),\n",
    "\n",
    "    # nn.Conv1d(256,512,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "\n",
    "    # nn.Conv1d(512,512,5),\n",
    "    # nn.ReLU(inplace=True),\n",
    "    # nn.MaxPool1d(2),\n",
    "    # nn.Dropout(0.5),\n",
    "\n",
    "    # nn.Flatten(),\n",
    "\n",
    "\n",
    "        )\n",
    "    \n",
    "        self.classifier =  nn.Sequential(\n",
    "    #             Reshape(2048,1),\n",
    "    # nn.LSTM(1, 2, 2,batch_first = True,bidirectional = True),\n",
    "    # extract_tensor(),\n",
    "    # nn.Flatten(),\n",
    "\n",
    "    nn.Linear(36000,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128,32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(32,self.cout),\n",
    "    nn.Softmax(dim=1))\n",
    "\n",
    "    def get_feature(self, inputs):\n",
    "\n",
    "        features = self.feature_extractor(inputs)\n",
    "        # print('feature shape:',features.shape)\n",
    "        return features \n",
    "\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        #outputs = self.seq_fft(inputs)\n",
    "\n",
    "        outputs = self.feature_extractor(inputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "        # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[0]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "        # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[1]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "        # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "        return self.dataloaders[2]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        inputs=inputs.reshape(inputs.shape[0],inputs.shape[1],1)\n",
    "        #print(inputs.shape)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # print(preds)\n",
    "        self.train_accuracy(preds, labels)\n",
    "\n",
    "        # print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"trainingh Loss:\",loss)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "\n",
    "        inputs=inputs.reshape(inputs.shape[0],inputs.shape[1],1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "        # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"VAL Loss:\",loss)\n",
    "        #self.val_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        # inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        inputs=inputs.reshape(inputs.shape[0],inputs.shape[1],1)\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        #print(preds)\n",
    "        self.test_f1_matrix = classification_report(labels.cpu(),preds.cpu())\n",
    "\n",
    "        # print(preds)\n",
    "\n",
    "        self.test_accuracy(preds, labels)\n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"val_f1\",self.val_accuracy.compute())\n",
    "        print(\"total val acc:\", self.val_accuracy.compute())\n",
    "        #print(self.val_f1_matrix)\n",
    "        print(\"*\" * 25)\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        self.log(\"training_f1\",self.train_accuracy.compute())\n",
    "        print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        print(\"total test acc:\", self.test_accuracy.compute())\n",
    "        print(self.test_f1_matrix)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decayRate = 0.96\n",
    "        optimizer = Adam(self.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    \n",
    "LR = 1e-4  \n",
    "BATCH_SIZE =30\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1018c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2ebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/lstm head\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params | In sizes     | Out sizes \n",
      "-----------------------------------------------------------------------------------\n",
      "0 | criterion         | CrossEntropyLoss | 0      | ?            | ?         \n",
      "1 | train_accuracy    | F1Score          | 0      | ?            | ?         \n",
      "2 | val_accuracy      | F1Score          | 0      | ?            | ?         \n",
      "3 | test_accuracy     | F1Score          | 0      | ?            | ?         \n",
      "4 | feature_extractor | Sequential       | 208    | [1, 9000, 1] | [1, 36000]\n",
      "5 | classifier        | Sequential       | 4.6 M  | [1, 36000]   | [1, 4]    \n",
      "-----------------------------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00,  2.17it/s]total val acc: tensor(0.1815, device='cuda:0')\n",
      "*************************\n",
      "Epoch 0:   2%|         | 10/530 [00:08<07:45,  1.12it/s, loss=1.39, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|| 57/57 [00:26<00:00,  2.15it/s]total val acc: tensor(0.4238, device='cuda:0')\n",
      "*************************\n",
      "Validation DataLoader 0: 100%|| 57/57 [00:26<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "     Validate metric           DataLoader 0\n",
      "\n",
      "         val_f1             0.42379117012023926\n",
      "        val_loss            1.3629827499389648\n",
      "\n",
      "Testing DataLoader 0: 100%|| 1/1 [00:00<00:00,  2.06it/s]total test acc: tensor(0.4077, device='cuda:0')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        72\n",
      "           1       0.56      1.00      0.72       475\n",
      "           2       0.00      0.00      0.00       275\n",
      "           3       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.56       841\n",
      "   macro avg       0.14      0.25      0.18       841\n",
      "weighted avg       0.32      0.56      0.41       841\n",
      "\n",
      "Testing DataLoader 0: 100%|| 1/1 [00:00<00:00,  2.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\",log_graph=True,name=\"lstm head\")\n",
    "model = MyModel(1,4, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "# print(model)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1, gpus=0,logger= logger)  #\n",
    "\n",
    "    # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "trainer.fit(model)\n",
    "# torch.save(model, \"tmp/saved_model\")\n",
    "trainer.validate(model)\n",
    "trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366c042",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6be65685-8d3f-4f09-ba5a-96daa092b5b7",
   "metadata": {},
   "source": [
    "## try model from paper with fft data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd6a67-9e3c-4f74-980d-83642998dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 7.0, 'N': 1.0, 'O': 2.0, '~': 26.0}\n",
      "[1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.13 ('wim': conda)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n wim ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "\n",
    "train_pd,  test_pd = train_test_split(meta_precessed_pd, test_size=0.3, random_state=42,stratify =meta_precessed_pd['encoded_label'] )\n",
    "# print(train_pd.head())\n",
    "train_pd_ros = data_ros(train_pd,cate_counts_dic )\n",
    "\n",
    "X_train_pd = train_pd_ros['preprocessed_data']\n",
    "y_train_pd = train_pd_ros['encoded_label']\n",
    "X_val_pd, X_test_pd, y_val_pd, y_test_pd = train_test_split(test_pd['preprocessed_data'], test_pd['encoded_label'], test_size=0.33, random_state=42,stratify = test_pd['encoded_label'])\n",
    "\n",
    "\n",
    "\n",
    "def pd_2array(X_pd,y_pd):\n",
    "    '''\n",
    "    Convert DataFrema to np.array\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        X_pd:X in datafrema form, each row is one np.array data\n",
    "        y_pd: y in datafrema form, each row is a single number\n",
    "    \n",
    "    return:\n",
    "        X,y in np.array form \n",
    "    '''\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    for X,y in zip( X_pd,y_pd):\n",
    "        X_.append(X.tolist())\n",
    "        y_.append(y) \n",
    "\n",
    "    return np.array(X_),np.array(y_)\n",
    "\n",
    "\n",
    "X_train , y_train = pd_2array(X_train_pd, y_train_pd)\n",
    "X_val , y_val = pd_2array(X_val_pd, y_val_pd)\n",
    "X_test , y_test = pd_2array(X_test_pd, y_test_pd)\n",
    "\n",
    "class ecg_Dataset(Dataset):\n",
    "    def __init__(self, X_pd,y_pd):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_pd = X_pd\n",
    "        self.y_pd = y_pd\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_pd)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X_pd.iloc[idx], self.y_pd.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07aefd-7f93-4fb0-ba4a-e0a7d9fbc983",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.13 ('wim': conda)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n wim ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cin,\n",
    "            cout,\n",
    "        k_size,\n",
    "\n",
    "            dataloaders,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.k_size = k_size,\n",
    "        self.cin= cin,\n",
    "        self.cout=cout\n",
    "        self.seq_fft = nn.Sequential(\n",
    "    nn.Conv1d(cin_fft,32,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,32,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(32,64,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,64,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(64,128,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,128,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(128,256,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(256,256,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Conv1d(256,512,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "\n",
    "    nn.Conv1d(512,512,k_size),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool1d(2),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1024,128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128,32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(32,4),\n",
    "    nn.Softmax(dim=1)\n",
    "        \n",
    "        )\n",
    "        \n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_accuracy = torchmetrics.F1Score()\n",
    "\n",
    "        self.val_accuracy = torchmetrics.F1Score()\n",
    "        self.test_accuracy = torchmetrics.F1Score()\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        outputs = self.seq_fft(inputs)\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "        # return DataLoader(ecg_train_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[0]\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "        # return DataLoader(ecg_val_dataset, batch_size=100)\n",
    "\n",
    "        return self.dataloaders[1]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "        # return DataLoader(ecg_test_dataset, batch_size=100)\n",
    "        return self.dataloaders[2]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        #print(inputs.shape)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # print(preds)\n",
    "        self.train_accuracy(preds, labels)\n",
    "\n",
    "        # print(\"training acc:\",self.train_accuracy(outputs, labels))\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"trainingh Loss:\",loss)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        # print(\"val acc:\",self.val_accuracy(outputs, labels))\n",
    "        # print(\"total val acc:\",self.val_accuracy.compute())\n",
    "\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        # print(\"VAL Loss:\",loss)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        inputs=inputs.reshape(inputs.shape[0],1,inputs.shape[1])\n",
    "\n",
    "        # inputs = inputs.long().view(inputs.size(0), -1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        outputs = self(inputs)\n",
    "        # outputs = torch.argmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # print(preds)\n",
    "\n",
    "        self.test_accuracy(preds, labels)\n",
    "    def validation_epoch_end(self, outs):\n",
    "        print(\"total val acc:\", self.val_accuracy.compute())\n",
    "        print(\"*\" * 25)\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        print(\"total training acc:\", self.train_accuracy.compute())\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, outs):\n",
    "        # print(   compute_epoch_loss_from_outputs(outs))\n",
    "        print(\"total test acc:\", self.test_accuracy.compute())\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decayRate = 0.96\n",
    "        optimizer = Adam(self.parameters(), lr=LR)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    \n",
    "LR = 1e-4  \n",
    "BATCH_SIZE =30\n",
    "ecg_train_dataset = ecg_Dataset(X_train_pd, y_train_pd)\n",
    "ecg_train_dataloader = DataLoader(ecg_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_val_dataset = ecg_Dataset(X_val_pd, y_val_pd)\n",
    "ecg_val_dataloader = DataLoader(ecg_val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "ecg_test_dataset = ecg_Dataset(X_test_pd, y_test_pd)\n",
    "ecg_test_dataloader = DataLoader(ecg_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "model = MyModel(1,4,3, [ecg_train_dataloader, ecg_val_dataloader, ecg_test_dataloader])\n",
    "# print(model)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=50, gpus=0)  #\n",
    "\n",
    "    # trainer = pl.Trainer(accelerator=\"gpu\", devices=1, max_epochs=1000, gpus=0,callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")]) #\n",
    "trainer.fit(model)\n",
    "# torch.save(model, \"tmp/saved_model\")\n",
    "trainer.validate(model)\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955253af-badc-4b82-a25f-f5c886ae7dac",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.13 ('wim': conda)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n wim ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "063ca1264bc21b360b86df46029e4774132bac2e793f5cd2e8ceda0c5254a661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
